"""
æ–‡æ¡£åŒæ­¥æœåŠ¡
è´Ÿè´£è‡ªåŠ¨åŒæ­¥å’Œæ›´æ–°å„ç§æ•°æ®æº
- å®˜æ–¹æ–‡æ¡£ï¼šReactã€Vueã€TypeScript ç­‰ï¼ˆæ¯å‘¨è‡ªåŠ¨æ›´æ–°ï¼‰
- GitHub æ–‡æ¡£ï¼šå…¬å¸å†…éƒ¨è§„èŒƒæ–‡æ¡£ï¼ˆå®æ—¶åŒæ­¥ï¼‰
- æ‰‹åŠ¨ä¸Šä¼ çš„æ–‡æ¡£ï¼šPDFã€Markdown ç­‰

ç±»æ¯”ï¼šè¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½å›¾ä¹¦é‡‡è´­å‘˜ï¼Œå®šæœŸå»å‡ºç‰ˆç¤¾å’Œä¹¦åº—é‡‡è´­æ–°ä¹¦
"""
import os
import re
import json
import time
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

from config import (
    OFFICIAL_SOURCES,
    GITHUB_REPO,
    GITHUB_TOKEN,
    DOCUMENTS_PATH
)
from database import get_vector_store
from document_processor import get_document_processor
from deepseek_client import get_embedding_client


class OfficialDocSyncer:
    """å®˜æ–¹æ–‡æ¡£åŒæ­¥å™¨"""

    def __init__(self):
        self.processor = get_document_processor()
        self.embedding_client = get_embedding_client()
        self.vector_store = get_vector_store()
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })

    def sync_source(self, source_key: str) -> Dict:
        """
        åŒæ­¥å•ä¸ªå®˜æ–¹æ–‡æ¡£æº

        Args:
            source_key: æ–‡æ¡£æºæ ‡è¯†ï¼ˆå¦‚ 'react', 'vue'ï¼‰

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        if source_key not in OFFICIAL_SOURCES:
            return {"error": f"æœªçŸ¥çš„æ–‡æ¡£æº: {source_key}"}

        config = OFFICIAL_SOURCES[source_key]
        print(f"ğŸ”„ å¼€å§‹åŒæ­¥: {config['name']}")

        # 1. è·å–é¡µé¢åˆ—è¡¨
        urls = self._get_page_urls(config)

        if not urls:
            return {"error": "æœªæ‰¾åˆ°å¯æŠ“å–çš„é¡µé¢"}

        print(f"ğŸ“„ å‘ç° {len(urls)} ä¸ªé¡µé¢")

        # 2. æ¸…ç†æ—§æ•°æ®
        self.vector_store.delete_by_source(source_key)
        print("ğŸ—‘ï¸ å·²æ¸…ç†æ—§æ•°æ®")

        # 3. æŠ“å–å¹¶å¤„ç†æ¯ä¸ªé¡µé¢
        success_count = 0
        fail_count = 0

        for i, url in enumerate(urls, 1):
            try:
                print(f"  [{i}/{len(urls)}] æŠ“å–: {url}")
                self._process_page(url, source_key, config)
                success_count += 1

                # ç¤¼è²Œæ€§å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)

            except Exception as e:
                print(f"  âš ï¸ æŠ“å–å¤±è´¥: {url} - {e}")
                fail_count += 1

        return {
            "source": source_key,
            "name": config["name"],
            "total": len(urls),
            "success": success_count,
            "failed": fail_count,
            "synced_at": datetime.now().isoformat()
        }

    def _get_page_urls(self, config: Dict) -> List[str]:
        """
        è·å–æ–‡æ¡£ç«™ç‚¹çš„æ‰€æœ‰é¡µé¢ URL

        ç­–ç•¥ï¼š
        1. å…ˆå°è¯• sitemap.xml
        2. å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä»é¦–é¡µæŠ“å–é“¾æ¥
        """
        base_url = config["base_url"]
        urls = []

        # å°è¯•è·å– sitemap
        sitemap_url = config.get("sitemap")
        if sitemap_url:
            try:
                response = self.session.get(sitemap_url, timeout=30)
                if response.status_code == 200:
                    # è§£æ XML sitemap
                    soup = BeautifulSoup(response.content, 'xml')
                    locs = soup.find_all('loc')
                    urls = [loc.text for loc in locs]
                    # è¿‡æ»¤åªä¿ç•™æ–‡æ¡£é¡µé¢
                    urls = [u for u in urls if self._is_doc_page(u, base_url)]
            except Exception as e:
                print(f"  âš ï¸ è·å– sitemap å¤±è´¥: {e}")

        # å¦‚æœ sitemap å¤±è´¥æˆ–ä¸ºç©ºï¼Œä»é¦–é¡µæŠ“å–
        if not urls:
            try:
                response = self.session.get(base_url, timeout=30)
                soup = BeautifulSoup(response.content, 'html.parser')

                # æŸ¥æ‰¾æ‰€æœ‰å†…éƒ¨é“¾æ¥
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    full_url = urljoin(base_url, href)

                    # åªä¿ç•™æ–‡æ¡£é¡µé¢
                    if self._is_doc_page(full_url, base_url):
                        urls.append(full_url)

                # å»é‡
                urls = list(set(urls))
            except Exception as e:
                print(f"  âš ï¸ ä»é¦–é¡µæŠ“å–å¤±è´¥: {e}")

        # é™åˆ¶æ•°é‡ï¼Œé¿å…æŠ“å–è¿‡å¤š
        return urls[:100]  # æœ€å¤šæŠ“ 100 é¡µ

    def _is_doc_page(self, url: str, base_url: str) -> bool:
        """åˆ¤æ–­ URL æ˜¯å¦æ˜¯æ–‡æ¡£é¡µé¢"""
        # å¿…é¡»æ˜¯åŒä¸€åŸŸåä¸‹çš„é¡µé¢
        if not url.startswith(base_url):
            return False

        # æ’é™¤éæ–‡æ¡£é¡µé¢
        excluded_patterns = [
            r'/blog/',
            r'/community/',
            r'/about/',
            r'/team/',
            r'\.pdf$',
            r'\.png$',
            r'\.jpg$',
            r'\.gif$',
            r'#',  # é”šç‚¹
        ]

        for pattern in excluded_patterns:
            if re.search(pattern, url):
                return False

        return True

    def _process_page(self, url: str, source_key: str, config: Dict):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        response = self.session.get(url, timeout=30)
        response.raise_for_status()

        html_content = response.text

        # ä½¿ç”¨æ–‡æ¡£å¤„ç†å™¨æå–å†…å®¹
        chunks = self.processor.process_webpage(
            url=url,
            html_content=html_content,
            metadata={"source_type": "official", "doc_source": source_key}
        )

        if not chunks:
            return

        # æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
        texts = [chunk["text"] for chunk in chunks]
        metadatas = [
            {
                **chunk["metadata"],
                "source": url,
                "doc_source": source_key
            }
            for chunk in chunks
        ]

        # ç”Ÿæˆ Embedding å¹¶å­˜å…¥æ•°æ®åº“
        embeddings = self.embedding_client.get_embeddings(texts)
        self.vector_store.add_documents(
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
            source_type="official"
        )

    def sync_all(self) -> List[Dict]:
        """åŒæ­¥æ‰€æœ‰é…ç½®çš„å®˜æ–¹æ–‡æ¡£æº"""
        results = []
        for source_key in OFFICIAL_SOURCES.keys():
            result = self.sync_source(source_key)
            results.append(result)
            time.sleep(1)  # æ–‡æ¡£æºä¹‹é—´å»¶è¿Ÿ
        return results


class GitHubSyncer:
    """GitHub æ–‡æ¡£åŒæ­¥å™¨"""

    def __init__(self, repo: str = None, token: str = None):
        self.repo = repo or GITHUB_REPO
        self.token = token or GITHUB_TOKEN
        self.processor = get_document_processor()
        self.embedding_client = get_embedding_client()
        self.vector_store = get_vector_store()
        self.api_base = "https://api.github.com"

    def sync_repo_docs(self) -> Dict:
        """
        åŒæ­¥ GitHub ä»“åº“ä¸­çš„æ–‡æ¡£

        ç­–ç•¥ï¼š
        1. è·å–ä»“åº“æ ¹ç›®å½•ä¸‹çš„ Markdown æ–‡ä»¶
        2. è·å– docs/ ç›®å½•ä¸‹çš„æ‰€æœ‰ Markdown æ–‡ä»¶
        3. å¤„ç†å¹¶å…¥åº“
        """
        if not self.repo:
            return {"error": "æœªé…ç½® GITHUB_REPO"}

        if not self.token:
            print("âš ï¸ æœªé…ç½® GITHUB_TOKENï¼Œå¯èƒ½ä¼šå—åˆ° API é€Ÿç‡é™åˆ¶")

        print(f"ğŸ”„ å¼€å§‹åŒæ­¥ GitHub ä»“åº“: {self.repo}")

        headers = {}
        if self.token:
            headers["Authorization"] = f"token {self.token}"

        # è·å–éœ€è¦åŒæ­¥çš„æ–‡ä»¶åˆ—è¡¨
        files_to_sync = []

        # 1. æ ¹ç›®å½•ä¸‹çš„ Markdown
        root_files = self._list_directory("", headers)
        for f in root_files:
            if f["name"].endswith('.md'):
                files_to_sync.append(f)

        # 2. docs ç›®å½•ä¸‹çš„ Markdown
        docs_files = self._list_directory("docs", headers)
        for f in docs_files:
            if f["name"].endswith('.md'):
                files_to_sync.append(f)

        if not files_to_sync:
            return {"error": "æœªæ‰¾åˆ° Markdown æ–‡æ¡£"}

        print(f"ğŸ“„ å‘ç° {len(files_to_sync)} ä¸ªæ–‡æ¡£æ–‡ä»¶")

        # æ¸…ç†æ—§æ•°æ®
        self.vector_store.delete_by_source(f"github:{self.repo}")

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        success_count = 0
        for f in files_to_sync:
            try:
                print(f"  ğŸ“¥ ä¸‹è½½: {f['path']}")
                content = self._download_file(f["download_url"], headers)

                # å¤„ç†æ–‡æ¡£
                metadata = {
                    "filename": f["name"],
                    "path": f["path"],
                    "source": f"github:{self.repo}/{f['path']}"
                }

                chunks = self.processor.process_github_readme(
                    repo=self.repo,
                    content=content,
                    metadata=metadata
                )

                if chunks:
                    texts = [c["text"] for c in chunks]
                    metadatas = [c["metadata"] for c in chunks]
                    embeddings = self.embedding_client.get_embeddings(texts)

                    self.vector_store.add_documents(
                        documents=texts,
                        embeddings=embeddings,
                        metadatas=metadatas,
                        source_type="github"
                    )
                    success_count += 1

                time.sleep(0.3)  # é¿å…è§¦å‘ GitHub é€Ÿç‡é™åˆ¶

            except Exception as e:
                print(f"  âš ï¸ å¤„ç†å¤±è´¥: {f['path']} - {e}")

        return {
            "repo": self.repo,
            "total_files": len(files_to_sync),
            "success": success_count,
            "synced_at": datetime.now().isoformat()
        }

    def _list_directory(self, path: str, headers: Dict) -> List[Dict]:
        """åˆ—å‡ºç›®å½•å†…å®¹"""
        url = f"{self.api_base}/repos/{self.repo}/contents/{path}"

        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code == 404:
            return []  # ç›®å½•ä¸å­˜åœ¨

        response.raise_for_status()
        return response.json()

    def _download_file(self, download_url: str, headers: Dict) -> str:
        """ä¸‹è½½æ–‡ä»¶å†…å®¹"""
        response = requests.get(download_url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text


class DocumentImporter:
    """æœ¬åœ°æ–‡æ¡£å¯¼å…¥å™¨"""

    def __init__(self):
        self.processor = get_document_processor()
        self.embedding_client = get_embedding_client()
        self.vector_store = get_vector_store()

    def import_file(self, file_path: str, metadata: Dict = None) -> Dict:
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        try:
            # å¤„ç†æ–‡ä»¶
            chunks = self.processor.process_file(file_path, metadata)

            if not chunks:
                return {"error": "æœªèƒ½ä»æ–‡ä»¶ä¸­æå–å†…å®¹"}

            # æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
            texts = [c["text"] for c in chunks]
            metadatas = [c["metadata"] for c in chunks]

            # ç”Ÿæˆ Embedding
            embeddings = self.embedding_client.get_embeddings(texts)

            # å­˜å…¥æ•°æ®åº“
            self.vector_store.add_documents(
                documents=texts,
                embeddings=embeddings,
                metadatas=metadatas,
                source_type="document"
            )

            return {
                "success": True,
                "file": file_path,
                "chunks": len(chunks),
                "total_chars": sum(len(t) for t in texts)
            }

        except Exception as e:
            return {"error": str(e), "file": file_path}

    def import_directory(self, dir_path: str) -> List[Dict]:
        """æ‰¹é‡å¯¼å…¥ç›®å½•ä¸­çš„æ‰€æœ‰æ”¯æŒæ–‡ä»¶"""
        results = []
        path = Path(dir_path)

        for ext in ['.md', '.markdown', '.txt', '.pdf']:
            for file_path in path.rglob(f'*{ext}'):
                result = self.import_file(str(file_path))
                results.append(result)

        return results


# ä¾¿æ·å‡½æ•°
def run_full_sync() -> Dict:
    """æ‰§è¡Œå®Œæ•´åŒæ­¥ï¼ˆå®˜æ–¹æ–‡æ¡£ + GitHubï¼‰"""
    results = {
        "official": [],
        "github": None,
        "timestamp": datetime.now().isoformat()
    }

    # åŒæ­¥å®˜æ–¹æ–‡æ¡£
    print("\n" + "="*50)
    print("ğŸ“š åŒæ­¥å®˜æ–¹æ–‡æ¡£")
    print("="*50)
    official_syncer = OfficialDocSyncer()
    results["official"] = official_syncer.sync_all()

    # åŒæ­¥ GitHub
    print("\n" + "="*50)
    print("ğŸ™ åŒæ­¥ GitHub æ–‡æ¡£")
    print("="*50)
    github_syncer = GitHubSyncer()
    results["github"] = github_syncer.sync_repo_docs()

    return results


if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    result = run_full_sync()
    print("\n" + "="*50)
    print("âœ… åŒæ­¥å®Œæˆ")
    print(json.dumps(result, indent=2, ensure_ascii=False))
